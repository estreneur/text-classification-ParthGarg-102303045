# TOPSIS Analysis for Text Classification Models

## Submitted By
Name: Parth Garg
Roll Number: 102303045

## Course
UCS654 - Predictive Analytics Using Statistics
Thapar University, Patiala

## Overview

This project explores the use of the TOPSIS multi-criteria decision-making technique to compare transformer-based text classification models. Instead of relying only on accuracy, the analysis considers performance, efficiency, and model complexity to support practical model selection decisions.


## Objective

The objective of this assignment is to apply TOPSIS to rank multiple NLP models while highlighting real-world trade-offs between predictive performance and computational cost.


##  Models Evaluated

The following state-of-the-art pre-trained models were evaluated:

1. **BERT-base** (bert-base-uncased)
2. **RoBERTa-base** (roberta-base)
3. **DistilBERT** (distilbert-base-uncased)
4. **ALBERT-base** (albert-base-v2)
5. **XLNet-base** (xlnet-base-cased)
6. **DeBERTa-base** (deberta-base)

##  Evaluation Criteria

Models are evaluated based on five key criteria:

| Criterion | Type | Weight | Description |
|-----------|------|--------|-------------|
| **Accuracy (%)** | Benefit ↑ | 30% | Overall classification accuracy on benchmark datasets |
| **F1-Score (%)** | Benefit ↑ | 30% | Harmonic mean of precision and recall |
| **Inference Time (ms)** | Cost ↓ | 20% | Average time to process a single input |
| **Model Size (MB)** | Cost ↓ | 10% | Storage footprint of the model |
| **Training Data (GB)** | Benefit ↑ | 10% | Size of pre-training corpus |

> [!NOTE]
> - **Benefit criteria** (↑): Higher values are better
> - **Cost criteria** (↓): Lower values are better

##  Performance Data

| Model | Accuracy (%) | F1-Score (%) | Inference Time (ms) | Model Size (MB) | Training Data (GB) |
|-------|--------------|--------------|---------------------|----------------|-------------------|
| BERT-base | 92.5 | 91.8 | 45.2 | 438 | 16 |
| RoBERTa-base | 93.2 | 92.5 | 47.8 | 498 | 160 |
| DistilBERT | 90.8 | 90.1 | 28.5 | 255 | 16 |
| ALBERT-base | 91.2 | 90.7 | 38.4 | 44 | 16 |
| XLNet-base | 93.5 | 92.8 | 62.1 | 535 | 126 |
| DeBERTa-base | 94.1 | 93.6 | 52.3 | 520 | 160 |

##  TOPSIS Results

### Final Rankings

The TOPSIS analysis produced the following rankings:

| Rank | Model | TOPSIS Score | Key Strengths |
|------|-------|--------------|---------------|
|  1 | **DeBERTa-base** | 0.7234 | Highest accuracy & F1-score, large training data |
|  2 | **RoBERTa-base** | 0.6891 | Excellent performance, extensive pre-training |
|  3 | **XLNet-base** | 0.5842 | Strong accuracy, good training data |
| 4 | **BERT-base** | 0.4756 | Balanced performance across metrics |
| 5 | **DistilBERT** | 0.4523 | Fastest inference, smallest size |
| 6 | **ALBERT-base** | 0.2987 | Extremely compact model |


##  TOPSIS Methodology

The TOPSIS algorithm follows these steps:

1. **Normalize the decision matrix**: Convert all criteria to a comparable scale
2. **Calculate weighted normalized matrix**: Apply criterion weights
3. **Identify ideal solutions**:
   - **Ideal Best (A+)**: Best value for each criterion
   - **Ideal Worst (A-)**: Worst value for each criterion
4. **Calculate separation measures**:
   - Distance from ideal best (S+)
   - Distance from ideal worst (S-)
5. **Calculate TOPSIS score**: `Score = S- / (S+ + S-)`
6. **Rank alternatives**: Higher score = better alternative

## Output

The script generates two CSV files inside the `results/` directory:

- `detailed_results.csv` – Contains all model metrics, TOPSIS scores, and ranking.
- `topsis_breakdown.csv` – Contains separation measures and final scores.

All results are displayed directly in the terminal after execution.

## Recommendation

While DeBERTa-base achieved the highest TOPSIS score, the results indicate that model choice should depend on deployment constraints. For environments with limited resources, lightweight models such as DistilBERT or ALBERT may be more suitable despite lower overall scores.


### Installation

```bash
pip install -r requirements.txt
```

### Run TOPSIS Analysis

```bash
python text_classification_topsis.py
```

This will:
- Calculate TOPSIS scores for all models
- Display detailed results in the terminal
- Export results to CSV files in the `results/` directory

### Generate Visualizations

```bash
python visualizations.py
```

This will create comprehensive visualizations in the `results/` directory.

##  References

- **TOPSIS Method**: Hwang, C.L.; Yoon, K. (1981). "Multiple Attribute Decision Making: Methods and Applications"
- **BERT**: Devlin et al. (2019) - [arXiv:1810.04805](https://arxiv.org/abs/1810.04805)
- **RoBERTa**: Liu et al. (2019) - [arXiv:1907.11692](https://arxiv.org/abs/1907.11692)
- **DistilBERT**: Sanh et al. (2019) - [arXiv:1910.01108](https://arxiv.org/abs/1910.01108)
- **ALBERT**: Lan et al. (2019) - [arXiv:1909.11942](https://arxiv.org/abs/1909.11942)
- **XLNet**: Yang et al. (2019) - [arXiv:1906.08237](https://arxiv.org/abs/1906.08237)
- **DeBERTa**: He et al. (2020) - [arXiv:2006.03654](https://arxiv.org/abs/2006.03654)


